<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Hand Gesture Recognition</title>

  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>

  <style>
    :root {
      --primary-color: #0077be;
      --secondary-color: #00ffff;
      --text-color: #ffffff;
      --background-color: #003366;
    }

    body {
      font-family: 'Roboto', sans-serif;
      margin: 0;
      padding: 0;
      background-color: var(--background-color);
      color: var(--text-color);
      display: flex;
      flex-direction: column;
      align-items: center;
      min-height: 100vh;
    }

    .container {
      width: 100%;
      max-width: 800px;
      padding: 20px;
      box-sizing: border-box;
    }

    h1 {
      color: var(--secondary-color);
      text-align: center;
      margin-bottom: 30px;
    }

    #liveView {
      position: relative;
      width: 100%;
      max-width: 640px;
      margin: 0 auto;
      border-radius: 10px;
      overflow: hidden;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }

    #webcam {
      width: 100%;
      height: auto;
      display: block;
    }

    #output_canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    #character-display {
      text-align: center;
      font-size: 24px;
      margin-top: 20px;
      background-color: var(--primary-color);
      padding: 10px;
      border-radius: 5px;
    }

    .word {
      display: inline-block;
      margin-right: 10px;
      background-color: var(--secondary-color);
      color: var(--background-color);
      padding: 5px 10px;
      border-radius: 3px;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Hand Gesture Recognition</h1>

    <div id="liveView">
      <video id="webcam" autoplay playsinline></video>
      <canvas id="output_canvas"></canvas>
    </div>

    <div id="character-display"></div>
  </div>

  <script type="module">
    import { HandLandmarker, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

    let handLandmarker = undefined;
    let gestureModel = undefined;
    let labelEncoder = undefined;
    let runningMode = "IMAGE";

    const video = document.getElementById("webcam");
    const canvasElement = document.getElementById("output_canvas");
    const canvasCtx = canvasElement.getContext("2d");
    const characterDisplay = document.getElementById("character-display");

    let currentWord = '';

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise((resolve) => {
        video.onloadedmetadata = () => {
          resolve(video);
        };
      });
    }

    const createHandLandmarker = async () => {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
      );

      handLandmarker = await HandLandmarker.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task",
          delegate: "GPU"
        },
        runningMode: runningMode,
        numHands: 2
      });
    };

    const loadGestureModel = async () => {
      try {
        gestureModel = await tf.loadLayersModel('/api/model/model.json');
        console.log('Gesture recognition model loaded!');
      } catch (error) {
        console.error('Error loading the TensorFlow model:', error);
      }
    };

    const loadLabelEncoder = async () => {
      labelEncoder = ['A', 'B', 'C', 'D', 'DEL', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'NOTHING', 'O', 'P', 'Q', 'R', 'S', 'SPACE', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'];
    };

    async function initializeApp() {
      await createHandLandmarker();
      await loadGestureModel();
      await loadLabelEncoder();
      await setupCamera();
      video.play();
      predictWebcam();
    }

    let lastVideoTime = -1;

    async function predictWebcam() {
      canvasElement.style.width = video.videoWidth + "px";
      canvasElement.style.height = video.videoHeight + "px";
      canvasElement.width = video.videoWidth;
      canvasElement.height = video.videoHeight;

      if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await handLandmarker.setOptions({ runningMode: "VIDEO" });
      }

      const startTimeMs = performance.now();
      if (lastVideoTime !== video.currentTime) {
        lastVideoTime = video.currentTime;
        const results = handLandmarker.detectForVideo(video, startTimeMs);

        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

        if (results.landmarks) {
          for (const landmarks of results.landmarks) {
            drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
              color: "#00FF00",
              lineWidth: 5
            });
            drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 2 });

            const handFeatures = extractHandFeatures(landmarks);
            const inputTensor = tf.tensor([handFeatures]);
            const prediction = gestureModel.predict(inputTensor);
            const predictedGestureIndex = prediction.argMax(-1).dataSync()[0];

            const predictedGesture = labelEncoder[predictedGestureIndex] || 'Unknown';
            updateCharacterDisplay(predictedGesture);
          }
        }
        canvasCtx.restore();
      }

      window.requestAnimationFrame(predictWebcam);
    }

    function extractHandFeatures(landmarks) {
      const features = [];
      for (let i = 0; i < landmarks.length; i++) {
        features.push(landmarks[i].x, landmarks[i].y, landmarks[i].z);
      }
      return features;
    }

    function updateCharacterDisplay(gesture) {
      if (gesture !== 'Unknown' && gesture !== 'NOTHING') {
        if (gesture === 'SPACE') {
          if (currentWord) {
            const wordElement = document.createElement('span');
            wordElement.className = 'word';
            wordElement.textContent = currentWord;
            characterDisplay.appendChild(wordElement);
            currentWord = '';
          }
        } else if (gesture === 'DEL') {
          if (currentWord) {
            currentWord = currentWord.slice(0, -1);
          } else if (characterDisplay.lastChild) {
            characterDisplay.removeChild(characterDisplay.lastChild);
          }
        } else {
          currentWord += gesture;
        }

        // Update the display
        const currentWordElement = document.getElementById('current-word');
        if (!currentWordElement) {
          const newCurrentWordElement = document.createElement('span');
          newCurrentWordElement.id = 'current-word';
          newCurrentWordElement.className = 'word';
          characterDisplay.appendChild(newCurrentWordElement);
          // characterDisplay.insertBefore(newCurrentWordElement);
        }
        document.getElementById('current-word').textContent = currentWord;

        // Limit the number of words displayed
        while (characterDisplay.childElementCount > 10) {
          characterDisplay.removeChild(characterDisplay.firstChild);
        }
      }
    }

    window.addEventListener('load', initializeApp);
  </script>
</body>
</html>